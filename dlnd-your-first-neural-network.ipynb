{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your first neural network\n",
    "\n",
    "In this project, you'll build your first neural network and use it to predict daily bike rental ridership. We've provided some of the code, but left the implementation of the neural network up to you (for the most part). After you've submitted this project, feel free to explore the data and the model more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare the data\n",
    "\n",
    "A critical step in working with neural networks is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights. Below, we've written the code to load and prepare the data. You'll learn more about this soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "\n",
    "rides = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking out the data\n",
    "\n",
    "This dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the `cnt` column. You can see the first few rows of the data above.\n",
    "\n",
    "Below is a plot showing the number of bike riders over the first 10 days in the data set. You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10c79c470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvgAAAIPCAYAAAAGtapCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113c5b5f8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 263,
       "width": 380
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rides[:24*10].plot(x='dteday', y='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy variables\n",
    "Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to `get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>...</th>\n",
       "      <th>hr_21</th>\n",
       "      <th>hr_22</th>\n",
       "      <th>hr_23</th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   yr  holiday  temp   hum  windspeed  casual  registered  cnt  season_1  \\\n",
       "0   0        0  0.24  0.81        0.0       3          13   16         1   \n",
       "1   0        0  0.22  0.80        0.0       8          32   40         1   \n",
       "2   0        0  0.22  0.80        0.0       5          27   32         1   \n",
       "3   0        0  0.24  0.75        0.0       3          10   13         1   \n",
       "4   0        0  0.24  0.75        0.0       0           1    1         1   \n",
       "\n",
       "   season_2    ...      hr_21  hr_22  hr_23  weekday_0  weekday_1  weekday_2  \\\n",
       "0         0    ...          0      0      0          0          0          0   \n",
       "1         0    ...          0      0      0          0          0          0   \n",
       "2         0    ...          0      0      0          0          0          0   \n",
       "3         0    ...          0      0      0          0          0          0   \n",
       "4         0    ...          0      0      0          0          0          0   \n",
       "\n",
       "   weekday_3  weekday_4  weekday_5  weekday_6  \n",
       "0          0          0          0          1  \n",
       "1          0          0          0          1  \n",
       "2          0          0          0          1  \n",
       "3          0          0          0          1  \n",
       "4          0          0          0          1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling target variables\n",
    "To make training the network easier, we'll standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1.\n",
    "\n",
    "The scaling factors are saved so we can go backwards when we use the network for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training, testing, and validation sets\n",
    "\n",
    "We'll save the last 21 days of the data to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual number of riders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the last 21 days \n",
    "test_data = data[-21*24:]\n",
    "data = data[:-21*24]\n",
    "\n",
    "# Separate the data into features and targets\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we'll train on historical data, then try to predict on future data (the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hold out the last 60 days of the remaining data as a validation set\n",
    "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
    "val_features, val_targets = features[-60*24:], targets[-60*24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to build the network\n",
    "\n",
    "Below you'll build your network. We've built out the structure and the backwards pass. You'll implement the forward pass through the network. You'll also set the hyperparameters: the learning rate, the number of hidden units, and the number of training passes.\n",
    "\n",
    "The network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is $f(x)=x$. A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called *forward propagation*.\n",
    "\n",
    "We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called *backpropagation*.\n",
    "\n",
    "> **Hint:** You'll need the derivative of the output activation function ($f(x) = x$) for the backpropagation implementation. If you aren't familiar with calculus, this function is equivalent to the equation $y = x$. What is the slope of that equation? That is the derivative of $f(x)$.\n",
    "\n",
    "Below, you have these tasks:\n",
    "1. Implement the sigmoid function to use as the activation function. Set `self.activation_function` in `__init__` to your sigmoid function.\n",
    "2. Implement the forward pass in the `train` method.\n",
    "3. Implement the backpropagation algorithm in the `train` method, including calculating the output error.\n",
    "4. Implement the forward pass in the `run` method.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.input_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                       (self.output_nodes, self.hidden_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### Set this to your implemented sigmoid function ####\n",
    "        # Activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # Convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        ### Forward pass ###\n",
    "        # TODO: Hidden layer\n",
    "        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs) # signals into hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer\n",
    "        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs) # signals into final output layer\n",
    "        final_outputs = final_inputs # signals from final output layer \n",
    "        \n",
    "        #### Implement the backward pass here ####\n",
    "        ### Backward pass ###\n",
    "        \n",
    "        # TODO: Output error\n",
    "        output_errors = targets - final_outputs # Output layer error is the difference between desired target and actual output.\n",
    "        #FS\n",
    "        error_gradient_output_layer = output_errors * 1\n",
    "        #print(\"error_gradient_output\", error_gradient_output_layer)\n",
    "        \n",
    "        # TODO: Backpropagated error\n",
    "        hidden_errors = self.weights_hidden_to_output * output_errors # errors propagated to the hidden layer\n",
    "        hidden_grad = hidden_outputs * (1-hidden_outputs) # hidden layer gradients\n",
    "        #FS\n",
    "        error_gradient_hidden_layers = error_gradient_output_layer * self.weights_hidden_to_output.T * hidden_outputs * (1 - hidden_outputs)\n",
    "        #print(\"hidden_outputs\", hidden_outputs)\n",
    "        #print(\"hidden_outputs*hidden_outputs\", hidden_outputs * (1-hidden_outputs))\n",
    "        #print(\"error_gradient_hidden\", error_gradient_hidden_layers)\n",
    "        \n",
    "        # TODO: Update the weights\n",
    "        #self.weights_hidden_to_output += self.lr * np.dot(output_errors, hidden_outputs.T) # update hidden-to-output weights with gradient descent step\n",
    "        self.weights_hidden_to_output += self.lr * np.dot(error_gradient_output_layer,hidden_outputs.T)\n",
    "        #self.weights_input_to_hidden += self.lr * np.dot(hidden_errors*hidden_grad, inputs.T) # update input-to-hidden weights with gradient descent step\n",
    "        self.weights_input_to_hidden += self.lr * error_gradient_hidden_layers * inputs.T\n",
    "        \n",
    "    def run(self, inputs_list):\n",
    "        # Run a forward pass through the network\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        print(inputs)\n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer\n",
    "        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)# signals into hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer\n",
    "        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs) # signals into final output layer\n",
    "        final_outputs = final_inputs # signals from final output layer \n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(y, Y):\n",
    "    return np.mean((y-Y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Here you'll set the hyperparameters for the network. The strategy here is to find hyperparameters such that the error on the training set is low, but you're not overfitting to the data. If you train the network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. That is, the loss on the validation set will start increasing as the training set loss drops.\n",
    "\n",
    "You'll also be using a method known as Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently. You'll learn more about SGD later.\n",
    "\n",
    "### Choose the number of epochs\n",
    "This is the number of times the dataset will pass through the network, each time updating the weights. As the number of epochs increases, the network becomes better and better at predicting the targets in the training set. You'll need to choose enough epochs to train the network well but not too many or you'll be overfitting.\n",
    "\n",
    "### Choose the learning rate\n",
    "This scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. A good choice to start at is 0.1. If the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge.\n",
    "\n",
    "### Choose the number of hidden nodes\n",
    "The more hidden nodes you have, the more accurate predictions the model will make. Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden units is too low, then the model won't have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden units you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.0% ... Training loss: 0.648 ... Validation loss: 1.043[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.11% ... Training loss: 0.526 ... Validation loss: 0.855[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.22% ... Training loss: 0.583 ... Validation loss: 0.829[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.33% ... Training loss: 0.495 ... Validation loss: 0.737[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.44% ... Training loss: 0.610 ... Validation loss: 0.782[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.55% ... Training loss: 0.668 ... Validation loss: 0.923[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.66% ... Training loss: 0.458 ... Validation loss: 0.647[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.77% ... Training loss: 0.445 ... Validation loss: 0.831[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 0.88% ... Training loss: 0.400 ... Validation loss: 0.661[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.0% ... Training loss: 0.372 ... Validation loss: 0.581[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.11% ... Training loss: 0.412 ... Validation loss: 0.618[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.22% ... Training loss: 0.597 ... Validation loss: 0.924[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.33% ... Training loss: 0.428 ... Validation loss: 0.705[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.44% ... Training loss: 0.354 ... Validation loss: 0.577[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.55% ... Training loss: 0.460 ... Validation loss: 0.649[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.66% ... Training loss: 0.420 ... Validation loss: 0.576[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.77% ... Training loss: 0.342 ... Validation loss: 0.487[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 1.88% ... Training loss: 0.570 ... Validation loss: 0.856[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.0% ... Training loss: 0.343 ... Validation loss: 0.531[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.11% ... Training loss: 0.327 ... Validation loss: 0.508[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.22% ... Training loss: 0.326 ... Validation loss: 0.495[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.33% ... Training loss: 0.387 ... Validation loss: 0.521[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.44% ... Training loss: 0.353 ... Validation loss: 0.544[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.55% ... Training loss: 0.393 ... Validation loss: 0.546[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.66% ... Training loss: 0.404 ... Validation loss: 0.600[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.77% ... Training loss: 0.339 ... Validation loss: 0.504[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 2.88% ... Training loss: 0.340 ... Validation loss: 0.525[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.0% ... Training loss: 0.535 ... Validation loss: 0.631[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.11% ... Training loss: 0.319 ... Validation loss: 0.478[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.22% ... Training loss: 0.333 ... Validation loss: 0.496[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.33% ... Training loss: 0.332 ... Validation loss: 0.484[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.44% ... Training loss: 0.359 ... Validation loss: 0.477[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.55% ... Training loss: 0.324 ... Validation loss: 0.515[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.66% ... Training loss: 0.319 ... Validation loss: 0.499[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.77% ... Training loss: 0.363 ... Validation loss: 0.541[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 3.88% ... Training loss: 0.328 ... Validation loss: 0.539[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.0% ... Training loss: 0.426 ... Validation loss: 0.526[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.11% ... Training loss: 0.335 ... Validation loss: 0.501[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.22% ... Training loss: 0.354 ... Validation loss: 0.477[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.33% ... Training loss: 0.303 ... Validation loss: 0.493[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.44% ... Training loss: 0.296 ... Validation loss: 0.458[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.55% ... Training loss: 0.356 ... Validation loss: 0.506[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.66% ... Training loss: 0.319 ... Validation loss: 0.491[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.77% ... Training loss: 0.302 ... Validation loss: 0.482[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 4.88% ... Training loss: 0.322 ... Validation loss: 0.502[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.0% ... Training loss: 0.301 ... Validation loss: 0.455[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.11% ... Training loss: 0.315 ... Validation loss: 0.492[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.22% ... Training loss: 0.299 ... Validation loss: 0.488[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.33% ... Training loss: 0.325 ... Validation loss: 0.503[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.44% ... Training loss: 0.292 ... Validation loss: 0.458[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.55% ... Training loss: 0.302 ... Validation loss: 0.464[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.66% ... Training loss: 0.329 ... Validation loss: 0.489[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.77% ... Training loss: 0.310 ... Validation loss: 0.511[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 5.88% ... Training loss: 0.310 ... Validation loss: 0.572[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.0% ... Training loss: 0.373 ... Validation loss: 0.715[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.11% ... Training loss: 0.328 ... Validation loss: 0.469[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.22% ... Training loss: 0.324 ... Validation loss: 0.512[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.33% ... Training loss: 0.285 ... Validation loss: 0.461[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.44% ... Training loss: 0.289 ... Validation loss: 0.487[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.55% ... Training loss: 0.281 ... Validation loss: 0.480[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.66% ... Training loss: 0.318 ... Validation loss: 0.505[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.77% ... Training loss: 0.310 ... Validation loss: 0.511[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 6.88% ... Training loss: 0.326 ... Validation loss: 0.528[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.0% ... Training loss: 0.325 ... Validation loss: 0.479[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.11% ... Training loss: 0.383 ... Validation loss: 0.601[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.22% ... Training loss: 0.298 ... Validation loss: 0.487[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.33% ... Training loss: 0.318 ... Validation loss: 0.572[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.44% ... Training loss: 0.284 ... Validation loss: 0.473[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.55% ... Training loss: 0.468 ... Validation loss: 0.897[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.66% ... Training loss: 0.293 ... Validation loss: 0.438[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.77% ... Training loss: 0.284 ... Validation loss: 0.429[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 7.88% ... Training loss: 0.354 ... Validation loss: 0.439[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.0% ... Training loss: 0.303 ... Validation loss: 0.493[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.11% ... Training loss: 0.308 ... Validation loss: 0.505[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.22% ... Training loss: 0.288 ... Validation loss: 0.456[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.33% ... Training loss: 0.303 ... Validation loss: 0.462[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.44% ... Training loss: 0.282 ... Validation loss: 0.454[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.55% ... Training loss: 0.277 ... Validation loss: 0.456[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.66% ... Training loss: 0.304 ... Validation loss: 0.484[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.77% ... Training loss: 0.296 ... Validation loss: 0.447[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 8.88% ... Training loss: 0.299 ... Validation loss: 0.448[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.0% ... Training loss: 0.273 ... Validation loss: 0.471[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.11% ... Training loss: 0.370 ... Validation loss: 0.474[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.22% ... Training loss: 0.274 ... Validation loss: 0.441[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.33% ... Training loss: 0.390 ... Validation loss: 0.514[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.44% ... Training loss: 0.264 ... Validation loss: 0.438[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.55% ... Training loss: 0.285 ... Validation loss: 0.462[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.66% ... Training loss: 0.310 ... Validation loss: 0.441[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.77% ... Training loss: 0.346 ... Validation loss: 0.553[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 9.88% ... Training loss: 0.278 ... Validation loss: 0.471[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.0% ... Training loss: 0.310 ... Validation loss: 0.433[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.1% ... Training loss: 0.290 ... Validation loss: 0.462[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.2% ... Training loss: 0.334 ... Validation loss: 0.498[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.3% ... Training loss: 0.283 ... Validation loss: 0.473[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.4% ... Training loss: 0.263 ... Validation loss: 0.449[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.5% ... Training loss: 0.312 ... Validation loss: 0.474[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.6% ... Training loss: 0.260 ... Validation loss: 0.442[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.7% ... Training loss: 0.275 ... Validation loss: 0.458[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 10.8% ... Training loss: 0.271 ... Validation loss: 0.448[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.0% ... Training loss: 0.261 ... Validation loss: 0.424[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.1% ... Training loss: 0.266 ... Validation loss: 0.439[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.2% ... Training loss: 0.310 ... Validation loss: 0.519[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.3% ... Training loss: 0.270 ... Validation loss: 0.503[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.4% ... Training loss: 0.308 ... Validation loss: 0.423[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.5% ... Training loss: 0.287 ... Validation loss: 0.476[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.6% ... Training loss: 0.275 ... Validation loss: 0.457[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.7% ... Training loss: 0.314 ... Validation loss: 0.479[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 11.8% ... Training loss: 0.258 ... Validation loss: 0.441[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.0% ... Training loss: 0.255 ... Validation loss: 0.427[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.1% ... Training loss: 0.256 ... Validation loss: 0.434[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.2% ... Training loss: 0.261 ... Validation loss: 0.432[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.3% ... Training loss: 0.261 ... Validation loss: 0.431[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.4% ... Training loss: 0.258 ... Validation loss: 0.444[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.5% ... Training loss: 0.263 ... Validation loss: 0.497[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.6% ... Training loss: 0.329 ... Validation loss: 0.443[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.7% ... Training loss: 0.259 ... Validation loss: 0.441[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 12.8% ... Training loss: 0.254 ... Validation loss: 0.428[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.0% ... Training loss: 0.244 ... Validation loss: 0.440[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.1% ... Training loss: 0.243 ... Validation loss: 0.444[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.2% ... Training loss: 0.265 ... Validation loss: 0.424[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.3% ... Training loss: 0.260 ... Validation loss: 0.418[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.4% ... Training loss: 0.319 ... Validation loss: 0.558[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.5% ... Training loss: 0.265 ... Validation loss: 0.481[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.6% ... Training loss: 0.244 ... Validation loss: 0.417[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.7% ... Training loss: 0.288 ... Validation loss: 0.473[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 13.8% ... Training loss: 0.238 ... Validation loss: 0.430[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.0% ... Training loss: 0.248 ... Validation loss: 0.438[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.1% ... Training loss: 0.239 ... Validation loss: 0.462[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.2% ... Training loss: 0.301 ... Validation loss: 0.460[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.3% ... Training loss: 0.267 ... Validation loss: 0.456[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.4% ... Training loss: 0.262 ... Validation loss: 0.458[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.5% ... Training loss: 0.330 ... Validation loss: 0.541[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.6% ... Training loss: 0.318 ... Validation loss: 0.517[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.7% ... Training loss: 0.238 ... Validation loss: 0.433[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 14.8% ... Training loss: 0.273 ... Validation loss: 0.452[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.0% ... Training loss: 0.235 ... Validation loss: 0.411[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.1% ... Training loss: 0.237 ... Validation loss: 0.420[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.2% ... Training loss: 0.272 ... Validation loss: 0.457[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.3% ... Training loss: 0.237 ... Validation loss: 0.412[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.4% ... Training loss: 0.260 ... Validation loss: 0.455[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.5% ... Training loss: 0.234 ... Validation loss: 0.398[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.6% ... Training loss: 0.238 ... Validation loss: 0.413[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.7% ... Training loss: 0.284 ... Validation loss: 0.483[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 15.8% ... Training loss: 0.239 ... Validation loss: 0.421[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.0% ... Training loss: 0.285 ... Validation loss: 0.499[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.1% ... Training loss: 0.265 ... Validation loss: 0.423[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.2% ... Training loss: 0.274 ... Validation loss: 0.426[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.3% ... Training loss: 0.250 ... Validation loss: 0.405[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.4% ... Training loss: 0.286 ... Validation loss: 0.432[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.5% ... Training loss: 0.228 ... Validation loss: 0.395[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.6% ... Training loss: 0.236 ... Validation loss: 0.388[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.7% ... Training loss: 0.264 ... Validation loss: 0.436[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 16.8% ... Training loss: 0.293 ... Validation loss: 0.466[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.0% ... Training loss: 0.254 ... Validation loss: 0.424[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.1% ... Training loss: 0.249 ... Validation loss: 0.429[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.2% ... Training loss: 0.253 ... Validation loss: 0.426[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.3% ... Training loss: 0.247 ... Validation loss: 0.409[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.4% ... Training loss: 0.265 ... Validation loss: 0.419[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.5% ... Training loss: 0.226 ... Validation loss: 0.385[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.6% ... Training loss: 0.245 ... Validation loss: 0.416[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.7% ... Training loss: 0.229 ... Validation loss: 0.412[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 17.8% ... Training loss: 0.241 ... Validation loss: 0.419[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.0% ... Training loss: 0.261 ... Validation loss: 0.377[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.1% ... Training loss: 0.222 ... Validation loss: 0.425[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.2% ... Training loss: 0.233 ... Validation loss: 0.402[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.3% ... Training loss: 0.246 ... Validation loss: 0.414[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.4% ... Training loss: 0.226 ... Validation loss: 0.419[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.5% ... Training loss: 0.232 ... Validation loss: 0.429[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.6% ... Training loss: 0.230 ... Validation loss: 0.423[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.7% ... Training loss: 0.231 ... Validation loss: 0.445[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 18.8% ... Training loss: 0.247 ... Validation loss: 0.439[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.0% ... Training loss: 0.234 ... Validation loss: 0.414[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.1% ... Training loss: 0.321 ... Validation loss: 0.479[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.2% ... Training loss: 0.222 ... Validation loss: 0.400[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.3% ... Training loss: 0.216 ... Validation loss: 0.402[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.4% ... Training loss: 0.246 ... Validation loss: 0.476[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.5% ... Training loss: 0.316 ... Validation loss: 0.508[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.6% ... Training loss: 0.261 ... Validation loss: 0.491[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.7% ... Training loss: 0.220 ... Validation loss: 0.398[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 19.8% ... Training loss: 0.220 ... Validation loss: 0.424[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.0% ... Training loss: 0.320 ... Validation loss: 0.501[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.1% ... Training loss: 0.213 ... Validation loss: 0.382[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.2% ... Training loss: 0.215 ... Validation loss: 0.374[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.3% ... Training loss: 0.231 ... Validation loss: 0.413[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.4% ... Training loss: 0.207 ... Validation loss: 0.384[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.5% ... Training loss: 0.212 ... Validation loss: 0.376[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.6% ... Training loss: 0.218 ... Validation loss: 0.370[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.7% ... Training loss: 0.215 ... Validation loss: 0.383[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 20.8% ... Training loss: 0.217 ... Validation loss: 0.382[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.0% ... Training loss: 0.206 ... Validation loss: 0.383[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.1% ... Training loss: 0.276 ... Validation loss: 0.444[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.2% ... Training loss: 0.212 ... Validation loss: 0.390[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.3% ... Training loss: 0.217 ... Validation loss: 0.406[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.4% ... Training loss: 0.207 ... Validation loss: 0.374[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.5% ... Training loss: 0.217 ... Validation loss: 0.371[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.6% ... Training loss: 0.255 ... Validation loss: 0.418[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.7% ... Training loss: 0.215 ... Validation loss: 0.379[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 21.8% ... Training loss: 0.226 ... Validation loss: 0.394[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.0% ... Training loss: 0.201 ... Validation loss: 0.351[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.1% ... Training loss: 0.201 ... Validation loss: 0.359[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.2% ... Training loss: 0.194 ... Validation loss: 0.372[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.3% ... Training loss: 0.221 ... Validation loss: 0.391[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.4% ... Training loss: 0.204 ... Validation loss: 0.359[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.5% ... Training loss: 0.206 ... Validation loss: 0.363[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.6% ... Training loss: 0.219 ... Validation loss: 0.387[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.7% ... Training loss: 0.193 ... Validation loss: 0.362[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 22.8% ... Training loss: 0.195 ... Validation loss: 0.360[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.0% ... Training loss: 0.215 ... Validation loss: 0.372[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.1% ... Training loss: 0.203 ... Validation loss: 0.340[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.2% ... Training loss: 0.215 ... Validation loss: 0.380[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.3% ... Training loss: 0.190 ... Validation loss: 0.347[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.4% ... Training loss: 0.199 ... Validation loss: 0.349[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.5% ... Training loss: 0.191 ... Validation loss: 0.355[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.6% ... Training loss: 0.205 ... Validation loss: 0.344[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.7% ... Training loss: 0.209 ... Validation loss: 0.355[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 23.8% ... Training loss: 0.187 ... Validation loss: 0.341[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.0% ... Training loss: 0.216 ... Validation loss: 0.377[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.1% ... Training loss: 0.224 ... Validation loss: 0.383[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.2% ... Training loss: 0.207 ... Validation loss: 0.358[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.3% ... Training loss: 0.191 ... Validation loss: 0.361[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.4% ... Training loss: 0.187 ... Validation loss: 0.327[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.5% ... Training loss: 0.199 ... Validation loss: 0.318[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.6% ... Training loss: 0.202 ... Validation loss: 0.343[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.7% ... Training loss: 0.184 ... Validation loss: 0.329[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 24.8% ... Training loss: 0.186 ... Validation loss: 0.320[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.0% ... Training loss: 0.194 ... Validation loss: 0.323[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.1% ... Training loss: 0.192 ... Validation loss: 0.368[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.2% ... Training loss: 0.204 ... Validation loss: 0.417[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.3% ... Training loss: 0.175 ... Validation loss: 0.356[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.4% ... Training loss: 0.171 ... Validation loss: 0.355[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.5% ... Training loss: 0.176 ... Validation loss: 0.350[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.6% ... Training loss: 0.173 ... Validation loss: 0.349[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.7% ... Training loss: 0.197 ... Validation loss: 0.408[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 25.8% ... Training loss: 0.180 ... Validation loss: 0.325[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.0% ... Training loss: 0.177 ... Validation loss: 0.356[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.1% ... Training loss: 0.176 ... Validation loss: 0.363[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.2% ... Training loss: 0.185 ... Validation loss: 0.345[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.3% ... Training loss: 0.168 ... Validation loss: 0.329[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.4% ... Training loss: 0.202 ... Validation loss: 0.382[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.5% ... Training loss: 0.204 ... Validation loss: 0.324[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.6% ... Training loss: 0.190 ... Validation loss: 0.325[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.7% ... Training loss: 0.185 ... Validation loss: 0.316[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 26.8% ... Training loss: 0.216 ... Validation loss: 0.364[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.0% ... Training loss: 0.174 ... Validation loss: 0.308[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.1% ... Training loss: 0.165 ... Validation loss: 0.322[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.2% ... Training loss: 0.162 ... Validation loss: 0.303[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.3% ... Training loss: 0.162 ... Validation loss: 0.296[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.4% ... Training loss: 0.199 ... Validation loss: 0.350[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.5% ... Training loss: 0.155 ... Validation loss: 0.288[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.6% ... Training loss: 0.183 ... Validation loss: 0.314[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.7% ... Training loss: 0.153 ... Validation loss: 0.286[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 27.8% ... Training loss: 0.166 ... Validation loss: 0.315[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.0% ... Training loss: 0.151 ... Validation loss: 0.278[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.1% ... Training loss: 0.157 ... Validation loss: 0.280[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.2% ... Training loss: 0.150 ... Validation loss: 0.281[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.3% ... Training loss: 0.172 ... Validation loss: 0.309[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.4% ... Training loss: 0.149 ... Validation loss: 0.272[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.5% ... Training loss: 0.164 ... Validation loss: 0.281[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.6% ... Training loss: 0.154 ... Validation loss: 0.301[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.7% ... Training loss: 0.157 ... Validation loss: 0.288[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 28.8% ... Training loss: 0.142 ... Validation loss: 0.264[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.0% ... Training loss: 0.148 ... Validation loss: 0.274[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.1% ... Training loss: 0.141 ... Validation loss: 0.263[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.2% ... Training loss: 0.157 ... Validation loss: 0.272[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.3% ... Training loss: 0.163 ... Validation loss: 0.286[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.4% ... Training loss: 0.155 ... Validation loss: 0.271[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.5% ... Training loss: 0.149 ... Validation loss: 0.273[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.6% ... Training loss: 0.192 ... Validation loss: 0.271[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.7% ... Training loss: 0.144 ... Validation loss: 0.267[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 29.8% ... Training loss: 0.143 ... Validation loss: 0.261[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.0% ... Training loss: 0.140 ... Validation loss: 0.270[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.1% ... Training loss: 0.152 ... Validation loss: 0.262[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.2% ... Training loss: 0.136 ... Validation loss: 0.252[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.3% ... Training loss: 0.146 ... Validation loss: 0.264[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.4% ... Training loss: 0.137 ... Validation loss: 0.248[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.5% ... Training loss: 0.152 ... Validation loss: 0.253[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.6% ... Training loss: 0.149 ... Validation loss: 0.260[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.7% ... Training loss: 0.148 ... Validation loss: 0.263[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 30.8% ... Training loss: 0.138 ... Validation loss: 0.259[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.0% ... Training loss: 0.166 ... Validation loss: 0.287[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.1% ... Training loss: 0.141 ... Validation loss: 0.272[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.2% ... Training loss: 0.137 ... Validation loss: 0.252[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.3% ... Training loss: 0.197 ... Validation loss: 0.279[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.4% ... Training loss: 0.152 ... Validation loss: 0.260[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.5% ... Training loss: 0.142 ... Validation loss: 0.257[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.6% ... Training loss: 0.131 ... Validation loss: 0.232[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.7% ... Training loss: 0.190 ... Validation loss: 0.282[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 31.8% ... Training loss: 0.152 ... Validation loss: 0.244[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.0% ... Training loss: 0.131 ... Validation loss: 0.228[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.1% ... Training loss: 0.166 ... Validation loss: 0.263[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.2% ... Training loss: 0.140 ... Validation loss: 0.255[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.3% ... Training loss: 0.132 ... Validation loss: 0.227[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.4% ... Training loss: 0.180 ... Validation loss: 0.260[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.5% ... Training loss: 0.128 ... Validation loss: 0.229[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.6% ... Training loss: 0.132 ... Validation loss: 0.219[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.7% ... Training loss: 0.143 ... Validation loss: 0.230[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 32.8% ... Training loss: 0.138 ... Validation loss: 0.257[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.0% ... Training loss: 0.170 ... Validation loss: 0.235[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.1% ... Training loss: 0.142 ... Validation loss: 0.258[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.2% ... Training loss: 0.130 ... Validation loss: 0.249[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.3% ... Training loss: 0.124 ... Validation loss: 0.231[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.4% ... Training loss: 0.121 ... Validation loss: 0.220[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.5% ... Training loss: 0.137 ... Validation loss: 0.229[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.6% ... Training loss: 0.119 ... Validation loss: 0.214[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.7% ... Training loss: 0.120 ... Validation loss: 0.225[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 33.8% ... Training loss: 0.123 ... Validation loss: 0.207[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.0% ... Training loss: 0.116 ... Validation loss: 0.213[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.1% ... Training loss: 0.114 ... Validation loss: 0.198[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.2% ... Training loss: 0.148 ... Validation loss: 0.220[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.3% ... Training loss: 0.138 ... Validation loss: 0.229[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.4% ... Training loss: 0.155 ... Validation loss: 0.238[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.5% ... Training loss: 0.119 ... Validation loss: 0.214[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.6% ... Training loss: 0.110 ... Validation loss: 0.209[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.7% ... Training loss: 0.123 ... Validation loss: 0.198[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 34.8% ... Training loss: 0.129 ... Validation loss: 0.242[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.0% ... Training loss: 0.124 ... Validation loss: 0.203[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.1% ... Training loss: 0.135 ... Validation loss: 0.246[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.2% ... Training loss: 0.116 ... Validation loss: 0.204[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.3% ... Training loss: 0.132 ... Validation loss: 0.219[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.4% ... Training loss: 0.128 ... Validation loss: 0.220[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.5% ... Training loss: 0.117 ... Validation loss: 0.209[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.6% ... Training loss: 0.156 ... Validation loss: 0.273[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.7% ... Training loss: 0.108 ... Validation loss: 0.209[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 35.8% ... Training loss: 0.109 ... Validation loss: 0.225[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.0% ... Training loss: 0.113 ... Validation loss: 0.200[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.1% ... Training loss: 0.136 ... Validation loss: 0.207[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.2% ... Training loss: 0.110 ... Validation loss: 0.192[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.3% ... Training loss: 0.117 ... Validation loss: 0.201[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.4% ... Training loss: 0.104 ... Validation loss: 0.192[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.5% ... Training loss: 0.106 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.6% ... Training loss: 0.108 ... Validation loss: 0.182[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.7% ... Training loss: 0.289 ... Validation loss: 0.275[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 36.8% ... Training loss: 0.120 ... Validation loss: 0.214[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.0% ... Training loss: 0.115 ... Validation loss: 0.195[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.1% ... Training loss: 0.106 ... Validation loss: 0.195[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.2% ... Training loss: 0.108 ... Validation loss: 0.194[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.3% ... Training loss: 0.112 ... Validation loss: 0.207[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.4% ... Training loss: 0.111 ... Validation loss: 0.194[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.5% ... Training loss: 0.110 ... Validation loss: 0.209[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.6% ... Training loss: 0.138 ... Validation loss: 0.222[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.7% ... Training loss: 0.122 ... Validation loss: 0.222[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 37.8% ... Training loss: 0.101 ... Validation loss: 0.196[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.0% ... Training loss: 0.160 ... Validation loss: 0.238[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.1% ... Training loss: 0.102 ... Validation loss: 0.184[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.2% ... Training loss: 0.163 ... Validation loss: 0.265[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.3% ... Training loss: 0.102 ... Validation loss: 0.185[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.4% ... Training loss: 0.151 ... Validation loss: 0.252[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.5% ... Training loss: 0.110 ... Validation loss: 0.207[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.6% ... Training loss: 0.103 ... Validation loss: 0.187[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.7% ... Training loss: 0.100 ... Validation loss: 0.189[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 38.8% ... Training loss: 0.113 ... Validation loss: 0.195[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.0% ... Training loss: 0.097 ... Validation loss: 0.190[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.1% ... Training loss: 0.106 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.2% ... Training loss: 0.098 ... Validation loss: 0.173[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.3% ... Training loss: 0.118 ... Validation loss: 0.230[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.4% ... Training loss: 0.111 ... Validation loss: 0.195[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.5% ... Training loss: 0.115 ... Validation loss: 0.190[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.6% ... Training loss: 0.095 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.7% ... Training loss: 0.099 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 39.8% ... Training loss: 0.108 ... Validation loss: 0.186[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.0% ... Training loss: 0.101 ... Validation loss: 0.171[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.1% ... Training loss: 0.112 ... Validation loss: 0.206[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.2% ... Training loss: 0.104 ... Validation loss: 0.183[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.3% ... Training loss: 0.107 ... Validation loss: 0.183[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.4% ... Training loss: 0.105 ... Validation loss: 0.211[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.5% ... Training loss: 0.115 ... Validation loss: 0.214[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.6% ... Training loss: 0.093 ... Validation loss: 0.184[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.7% ... Training loss: 0.110 ... Validation loss: 0.179[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 40.8% ... Training loss: 0.090 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.0% ... Training loss: 0.093 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.1% ... Training loss: 0.114 ... Validation loss: 0.181[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.2% ... Training loss: 0.111 ... Validation loss: 0.205[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.3% ... Training loss: 0.099 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.4% ... Training loss: 0.113 ... Validation loss: 0.174[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.5% ... Training loss: 0.104 ... Validation loss: 0.186[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.6% ... Training loss: 0.133 ... Validation loss: 0.185[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.7% ... Training loss: 0.096 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 41.8% ... Training loss: 0.105 ... Validation loss: 0.203[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.0% ... Training loss: 0.095 ... Validation loss: 0.178[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.1% ... Training loss: 0.092 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.2% ... Training loss: 0.092 ... Validation loss: 0.171[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.3% ... Training loss: 0.098 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.4% ... Training loss: 0.111 ... Validation loss: 0.192[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.5% ... Training loss: 0.089 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.6% ... Training loss: 0.107 ... Validation loss: 0.181[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.7% ... Training loss: 0.108 ... Validation loss: 0.177[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 42.8% ... Training loss: 0.100 ... Validation loss: 0.180[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.0% ... Training loss: 0.106 ... Validation loss: 0.249[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.1% ... Training loss: 0.118 ... Validation loss: 0.190[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.2% ... Training loss: 0.102 ... Validation loss: 0.183[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.3% ... Training loss: 0.094 ... Validation loss: 0.176[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.4% ... Training loss: 0.090 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.5% ... Training loss: 0.109 ... Validation loss: 0.182[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.6% ... Training loss: 0.089 ... Validation loss: 0.178[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.7% ... Training loss: 0.105 ... Validation loss: 0.185[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 43.8% ... Training loss: 0.099 ... Validation loss: 0.192[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.0% ... Training loss: 0.090 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.1% ... Training loss: 0.085 ... Validation loss: 0.174[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.2% ... Training loss: 0.086 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.3% ... Training loss: 0.089 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.4% ... Training loss: 0.088 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.5% ... Training loss: 0.093 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.6% ... Training loss: 0.088 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.7% ... Training loss: 0.094 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 44.8% ... Training loss: 0.097 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.0% ... Training loss: 0.102 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.1% ... Training loss: 0.104 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.2% ... Training loss: 0.093 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.3% ... Training loss: 0.082 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.4% ... Training loss: 0.102 ... Validation loss: 0.192[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.5% ... Training loss: 0.092 ... Validation loss: 0.173[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.6% ... Training loss: 0.102 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.7% ... Training loss: 0.085 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 45.8% ... Training loss: 0.090 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.0% ... Training loss: 0.089 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.1% ... Training loss: 0.105 ... Validation loss: 0.177[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.2% ... Training loss: 0.090 ... Validation loss: 0.177[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.3% ... Training loss: 0.092 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.4% ... Training loss: 0.085 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.5% ... Training loss: 0.104 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.6% ... Training loss: 0.119 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.7% ... Training loss: 0.113 ... Validation loss: 0.244[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 46.8% ... Training loss: 0.092 ... Validation loss: 0.196[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.0% ... Training loss: 0.107 ... Validation loss: 0.220[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.1% ... Training loss: 0.093 ... Validation loss: 0.176[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.2% ... Training loss: 0.085 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.3% ... Training loss: 0.108 ... Validation loss: 0.186[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.4% ... Training loss: 0.104 ... Validation loss: 0.174[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.5% ... Training loss: 0.090 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.6% ... Training loss: 0.089 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.7% ... Training loss: 0.091 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 47.8% ... Training loss: 0.082 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.0% ... Training loss: 0.110 ... Validation loss: 0.191[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.1% ... Training loss: 0.117 ... Validation loss: 0.228[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.2% ... Training loss: 0.096 ... Validation loss: 0.213[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.3% ... Training loss: 0.102 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.4% ... Training loss: 0.084 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.5% ... Training loss: 0.088 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.6% ... Training loss: 0.097 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.7% ... Training loss: 0.082 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 48.8% ... Training loss: 0.080 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.0% ... Training loss: 0.087 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.1% ... Training loss: 0.081 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.2% ... Training loss: 0.103 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.3% ... Training loss: 0.097 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.4% ... Training loss: 0.128 ... Validation loss: 0.206[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.5% ... Training loss: 0.084 ... Validation loss: 0.171[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.6% ... Training loss: 0.115 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.7% ... Training loss: 0.100 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 49.8% ... Training loss: 0.090 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.0% ... Training loss: 0.082 ... Validation loss: 0.146[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.1% ... Training loss: 0.086 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.2% ... Training loss: 0.093 ... Validation loss: 0.178[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.3% ... Training loss: 0.088 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.4% ... Training loss: 0.093 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.5% ... Training loss: 0.087 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.6% ... Training loss: 0.085 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.7% ... Training loss: 0.086 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 50.8% ... Training loss: 0.079 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.0% ... Training loss: 0.102 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.1% ... Training loss: 0.088 ... Validation loss: 0.210[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.2% ... Training loss: 0.112 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.3% ... Training loss: 0.082 ... Validation loss: 0.185[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.4% ... Training loss: 0.090 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.5% ... Training loss: 0.083 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.6% ... Training loss: 0.102 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.7% ... Training loss: 0.099 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 51.8% ... Training loss: 0.085 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.0% ... Training loss: 0.078 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.1% ... Training loss: 0.079 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.2% ... Training loss: 0.081 ... Validation loss: 0.173[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.3% ... Training loss: 0.088 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.4% ... Training loss: 0.083 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.5% ... Training loss: 0.080 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.6% ... Training loss: 0.080 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.7% ... Training loss: 0.078 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 52.8% ... Training loss: 0.077 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.0% ... Training loss: 0.079 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.1% ... Training loss: 0.086 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.2% ... Training loss: 0.101 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.3% ... Training loss: 0.080 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.4% ... Training loss: 0.105 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.5% ... Training loss: 0.076 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.6% ... Training loss: 0.077 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.7% ... Training loss: 0.078 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 53.8% ... Training loss: 0.092 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.0% ... Training loss: 0.081 ... Validation loss: 0.173[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.1% ... Training loss: 0.077 ... Validation loss: 0.213[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.2% ... Training loss: 0.104 ... Validation loss: 0.192[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.3% ... Training loss: 0.085 ... Validation loss: 0.178[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.4% ... Training loss: 0.090 ... Validation loss: 0.200[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.5% ... Training loss: 0.089 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.6% ... Training loss: 0.083 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.7% ... Training loss: 0.084 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 54.8% ... Training loss: 0.079 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.0% ... Training loss: 0.085 ... Validation loss: 0.176[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.1% ... Training loss: 0.085 ... Validation loss: 0.180[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.2% ... Training loss: 0.113 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.3% ... Training loss: 0.081 ... Validation loss: 0.187[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.4% ... Training loss: 0.081 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.5% ... Training loss: 0.088 ... Validation loss: 0.182[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.6% ... Training loss: 0.092 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.7% ... Training loss: 0.084 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 55.8% ... Training loss: 0.104 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.0% ... Training loss: 0.095 ... Validation loss: 0.213[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.1% ... Training loss: 0.078 ... Validation loss: 0.171[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.2% ... Training loss: 0.082 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.3% ... Training loss: 0.076 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.4% ... Training loss: 0.075 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.5% ... Training loss: 0.076 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.6% ... Training loss: 0.076 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.7% ... Training loss: 0.099 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 56.8% ... Training loss: 0.081 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.0% ... Training loss: 0.081 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.1% ... Training loss: 0.081 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.2% ... Training loss: 0.101 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.3% ... Training loss: 0.093 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.4% ... Training loss: 0.090 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.5% ... Training loss: 0.079 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.6% ... Training loss: 0.082 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.7% ... Training loss: 0.073 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 57.8% ... Training loss: 0.073 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.0% ... Training loss: 0.080 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.1% ... Training loss: 0.138 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.2% ... Training loss: 0.079 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.3% ... Training loss: 0.079 ... Validation loss: 0.171[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.4% ... Training loss: 0.076 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.5% ... Training loss: 0.076 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.6% ... Training loss: 0.074 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.7% ... Training loss: 0.143 ... Validation loss: 0.207[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 58.8% ... Training loss: 0.079 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.0% ... Training loss: 0.082 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.1% ... Training loss: 0.082 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.2% ... Training loss: 0.074 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.3% ... Training loss: 0.078 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.4% ... Training loss: 0.074 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.5% ... Training loss: 0.083 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.6% ... Training loss: 0.074 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.7% ... Training loss: 0.073 ... Validation loss: 0.173[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 59.8% ... Training loss: 0.078 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.0% ... Training loss: 0.080 ... Validation loss: 0.196[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.1% ... Training loss: 0.076 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.2% ... Training loss: 0.083 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.3% ... Training loss: 0.092 ... Validation loss: 0.208[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.4% ... Training loss: 0.087 ... Validation loss: 0.214[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.5% ... Training loss: 0.074 ... Validation loss: 0.174[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.6% ... Training loss: 0.081 ... Validation loss: 0.219[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.7% ... Training loss: 0.072 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 60.8% ... Training loss: 0.076 ... Validation loss: 0.179[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.0% ... Training loss: 0.078 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.1% ... Training loss: 0.084 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.2% ... Training loss: 0.074 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.3% ... Training loss: 0.071 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.4% ... Training loss: 0.077 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.5% ... Training loss: 0.087 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.6% ... Training loss: 0.076 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.7% ... Training loss: 0.072 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 61.8% ... Training loss: 0.070 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.0% ... Training loss: 0.079 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.1% ... Training loss: 0.093 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.2% ... Training loss: 0.075 ... Validation loss: 0.173[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.3% ... Training loss: 0.074 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.4% ... Training loss: 0.076 ... Validation loss: 0.180[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.5% ... Training loss: 0.082 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.6% ... Training loss: 0.078 ... Validation loss: 0.182[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.7% ... Training loss: 0.077 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 62.8% ... Training loss: 0.085 ... Validation loss: 0.199[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.0% ... Training loss: 0.075 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.1% ... Training loss: 0.073 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.2% ... Training loss: 0.077 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.3% ... Training loss: 0.082 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.4% ... Training loss: 0.080 ... Validation loss: 0.182[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.5% ... Training loss: 0.072 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.6% ... Training loss: 0.078 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.7% ... Training loss: 0.076 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 63.8% ... Training loss: 0.074 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.0% ... Training loss: 0.083 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.1% ... Training loss: 0.079 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.2% ... Training loss: 0.071 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.3% ... Training loss: 0.080 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.4% ... Training loss: 0.074 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.5% ... Training loss: 0.085 ... Validation loss: 0.191[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.6% ... Training loss: 0.072 ... Validation loss: 0.141[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.7% ... Training loss: 0.117 ... Validation loss: 0.265[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 64.8% ... Training loss: 0.069 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.0% ... Training loss: 0.071 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.1% ... Training loss: 0.093 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.2% ... Training loss: 0.072 ... Validation loss: 0.213[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.3% ... Training loss: 0.071 ... Validation loss: 0.181[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.4% ... Training loss: 0.074 ... Validation loss: 0.177[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.5% ... Training loss: 0.073 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.6% ... Training loss: 0.071 ... Validation loss: 0.145[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.7% ... Training loss: 0.085 ... Validation loss: 0.194[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 65.8% ... Training loss: 0.081 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.0% ... Training loss: 0.089 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.1% ... Training loss: 0.076 ... Validation loss: 0.207[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.2% ... Training loss: 0.073 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.3% ... Training loss: 0.093 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.4% ... Training loss: 0.077 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.5% ... Training loss: 0.072 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.6% ... Training loss: 0.076 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.7% ... Training loss: 0.072 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 66.8% ... Training loss: 0.069 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.0% ... Training loss: 0.079 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.1% ... Training loss: 0.078 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.2% ... Training loss: 0.104 ... Validation loss: 0.197[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.3% ... Training loss: 0.071 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.4% ... Training loss: 0.084 ... Validation loss: 0.195[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.5% ... Training loss: 0.082 ... Validation loss: 0.181[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.6% ... Training loss: 0.073 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.7% ... Training loss: 0.095 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 67.8% ... Training loss: 0.072 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.0% ... Training loss: 0.075 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.1% ... Training loss: 0.086 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.2% ... Training loss: 0.074 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.3% ... Training loss: 0.071 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.4% ... Training loss: 0.086 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.5% ... Training loss: 0.078 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.6% ... Training loss: 0.071 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.7% ... Training loss: 0.070 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 68.8% ... Training loss: 0.076 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.0% ... Training loss: 0.121 ... Validation loss: 0.192[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.1% ... Training loss: 0.075 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.2% ... Training loss: 0.077 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.3% ... Training loss: 0.075 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.4% ... Training loss: 0.068 ... Validation loss: 0.143[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.5% ... Training loss: 0.132 ... Validation loss: 0.226[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.6% ... Training loss: 0.078 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.7% ... Training loss: 0.073 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 69.8% ... Training loss: 0.073 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.0% ... Training loss: 0.069 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.1% ... Training loss: 0.076 ... Validation loss: 0.151[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.2% ... Training loss: 0.072 ... Validation loss: 0.161[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.3% ... Training loss: 0.083 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.4% ... Training loss: 0.071 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.5% ... Training loss: 0.070 ... Validation loss: 0.151[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.6% ... Training loss: 0.073 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.7% ... Training loss: 0.070 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 70.8% ... Training loss: 0.066 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.0% ... Training loss: 0.081 ... Validation loss: 0.182[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.1% ... Training loss: 0.074 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.2% ... Training loss: 0.079 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.3% ... Training loss: 0.080 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.4% ... Training loss: 0.069 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.5% ... Training loss: 0.070 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.6% ... Training loss: 0.073 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.7% ... Training loss: 0.083 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 71.8% ... Training loss: 0.073 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.0% ... Training loss: 0.069 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.1% ... Training loss: 0.082 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.2% ... Training loss: 0.088 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.3% ... Training loss: 0.080 ... Validation loss: 0.181[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.4% ... Training loss: 0.090 ... Validation loss: 0.231[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.5% ... Training loss: 0.067 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.6% ... Training loss: 0.076 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.7% ... Training loss: 0.075 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 72.8% ... Training loss: 0.080 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.0% ... Training loss: 0.067 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.1% ... Training loss: 0.089 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.2% ... Training loss: 0.069 ... Validation loss: 0.176[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.3% ... Training loss: 0.093 ... Validation loss: 0.168[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.4% ... Training loss: 0.071 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.5% ... Training loss: 0.070 ... Validation loss: 0.143[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.6% ... Training loss: 0.071 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.7% ... Training loss: 0.069 ... Validation loss: 0.177[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 73.8% ... Training loss: 0.066 ... Validation loss: 0.151[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.0% ... Training loss: 0.077 ... Validation loss: 0.184[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.1% ... Training loss: 0.067 ... Validation loss: 0.144[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.2% ... Training loss: 0.077 ... Validation loss: 0.143[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.3% ... Training loss: 0.071 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.4% ... Training loss: 0.076 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.5% ... Training loss: 0.085 ... Validation loss: 0.243[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.6% ... Training loss: 0.071 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.7% ... Training loss: 0.065 ... Validation loss: 0.168[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 74.8% ... Training loss: 0.064 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.0% ... Training loss: 0.070 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.1% ... Training loss: 0.088 ... Validation loss: 0.207[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.2% ... Training loss: 0.069 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.3% ... Training loss: 0.072 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.4% ... Training loss: 0.073 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.5% ... Training loss: 0.063 ... Validation loss: 0.140[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.6% ... Training loss: 0.065 ... Validation loss: 0.141[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.7% ... Training loss: 0.074 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 75.8% ... Training loss: 0.067 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.0% ... Training loss: 0.063 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.1% ... Training loss: 0.066 ... Validation loss: 0.168[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.2% ... Training loss: 0.101 ... Validation loss: 0.250[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.3% ... Training loss: 0.066 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.4% ... Training loss: 0.071 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.5% ... Training loss: 0.129 ... Validation loss: 0.195[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.6% ... Training loss: 0.071 ... Validation loss: 0.146[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.7% ... Training loss: 0.084 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 76.8% ... Training loss: 0.068 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.0% ... Training loss: 0.073 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.1% ... Training loss: 0.067 ... Validation loss: 0.144[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.2% ... Training loss: 0.070 ... Validation loss: 0.138[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.3% ... Training loss: 0.067 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.4% ... Training loss: 0.067 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.5% ... Training loss: 0.069 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.6% ... Training loss: 0.072 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.7% ... Training loss: 0.073 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 77.8% ... Training loss: 0.074 ... Validation loss: 0.141[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.0% ... Training loss: 0.069 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.1% ... Training loss: 0.077 ... Validation loss: 0.138[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.2% ... Training loss: 0.067 ... Validation loss: 0.138[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.3% ... Training loss: 0.071 ... Validation loss: 0.151[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.4% ... Training loss: 0.066 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.5% ... Training loss: 0.074 ... Validation loss: 0.139[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.6% ... Training loss: 0.066 ... Validation loss: 0.139[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.7% ... Training loss: 0.100 ... Validation loss: 0.211[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 78.8% ... Training loss: 0.072 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.0% ... Training loss: 0.069 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.1% ... Training loss: 0.073 ... Validation loss: 0.187[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.2% ... Training loss: 0.066 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.3% ... Training loss: 0.073 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.4% ... Training loss: 0.065 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.5% ... Training loss: 0.079 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.6% ... Training loss: 0.094 ... Validation loss: 0.244[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.7% ... Training loss: 0.077 ... Validation loss: 0.228[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 79.8% ... Training loss: 0.069 ... Validation loss: 0.187[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.0% ... Training loss: 0.065 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.1% ... Training loss: 0.070 ... Validation loss: 0.191[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.2% ... Training loss: 0.074 ... Validation loss: 0.174[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.3% ... Training loss: 0.068 ... Validation loss: 0.186[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.4% ... Training loss: 0.066 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.5% ... Training loss: 0.074 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.6% ... Training loss: 0.063 ... Validation loss: 0.137[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.7% ... Training loss: 0.065 ... Validation loss: 0.138[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 80.8% ... Training loss: 0.072 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.0% ... Training loss: 0.065 ... Validation loss: 0.137[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.1% ... Training loss: 0.063 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.2% ... Training loss: 0.066 ... Validation loss: 0.145[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.3% ... Training loss: 0.078 ... Validation loss: 0.228[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.4% ... Training loss: 0.069 ... Validation loss: 0.195[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.5% ... Training loss: 0.077 ... Validation loss: 0.172[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.6% ... Training loss: 0.073 ... Validation loss: 0.183[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.7% ... Training loss: 0.088 ... Validation loss: 0.212[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 81.8% ... Training loss: 0.063 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.0% ... Training loss: 0.071 ... Validation loss: 0.145[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.1% ... Training loss: 0.065 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.2% ... Training loss: 0.064 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.3% ... Training loss: 0.066 ... Validation loss: 0.139[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.4% ... Training loss: 0.070 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.5% ... Training loss: 0.064 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.6% ... Training loss: 0.068 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.7% ... Training loss: 0.070 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 82.8% ... Training loss: 0.063 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.0% ... Training loss: 0.076 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.1% ... Training loss: 0.077 ... Validation loss: 0.219[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.2% ... Training loss: 0.081 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.3% ... Training loss: 0.077 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.4% ... Training loss: 0.075 ... Validation loss: 0.184[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.5% ... Training loss: 0.087 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.6% ... Training loss: 0.066 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.7% ... Training loss: 0.066 ... Validation loss: 0.168[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 83.8% ... Training loss: 0.074 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.0% ... Training loss: 0.062 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.1% ... Training loss: 0.070 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.2% ... Training loss: 0.138 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.3% ... Training loss: 0.072 ... Validation loss: 0.228[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.4% ... Training loss: 0.068 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.5% ... Training loss: 0.069 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.6% ... Training loss: 0.091 ... Validation loss: 0.215[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.7% ... Training loss: 0.069 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 84.8% ... Training loss: 0.070 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.0% ... Training loss: 0.065 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.1% ... Training loss: 0.073 ... Validation loss: 0.226[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.2% ... Training loss: 0.066 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.3% ... Training loss: 0.080 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.4% ... Training loss: 0.069 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.5% ... Training loss: 0.061 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.6% ... Training loss: 0.068 ... Validation loss: 0.168[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.7% ... Training loss: 0.066 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 85.8% ... Training loss: 0.063 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.0% ... Training loss: 0.066 ... Validation loss: 0.151[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.1% ... Training loss: 0.068 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.2% ... Training loss: 0.064 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.3% ... Training loss: 0.066 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.4% ... Training loss: 0.067 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.5% ... Training loss: 0.067 ... Validation loss: 0.145[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.6% ... Training loss: 0.068 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.7% ... Training loss: 0.071 ... Validation loss: 0.174[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 86.8% ... Training loss: 0.066 ... Validation loss: 0.145[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.0% ... Training loss: 0.065 ... Validation loss: 0.146[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.1% ... Training loss: 0.068 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.2% ... Training loss: 0.067 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.3% ... Training loss: 0.088 ... Validation loss: 0.240[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.4% ... Training loss: 0.070 ... Validation loss: 0.151[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.5% ... Training loss: 0.063 ... Validation loss: 0.146[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.6% ... Training loss: 0.062 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.7% ... Training loss: 0.080 ... Validation loss: 0.184[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 87.8% ... Training loss: 0.064 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.0% ... Training loss: 0.062 ... Validation loss: 0.140[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.1% ... Training loss: 0.072 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.2% ... Training loss: 0.066 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.3% ... Training loss: 0.069 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.4% ... Training loss: 0.066 ... Validation loss: 0.171[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.5% ... Training loss: 0.073 ... Validation loss: 0.154[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.6% ... Training loss: 0.066 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.7% ... Training loss: 0.067 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 88.8% ... Training loss: 0.063 ... Validation loss: 0.144[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.0% ... Training loss: 0.067 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.1% ... Training loss: 0.070 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.2% ... Training loss: 0.074 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.3% ... Training loss: 0.067 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.4% ... Training loss: 0.061 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.5% ... Training loss: 0.100 ... Validation loss: 0.200[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.6% ... Training loss: 0.061 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.7% ... Training loss: 0.075 ... Validation loss: 0.178[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 89.8% ... Training loss: 0.099 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.0% ... Training loss: 0.064 ... Validation loss: 0.176[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.1% ... Training loss: 0.085 ... Validation loss: 0.140[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.2% ... Training loss: 0.066 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.3% ... Training loss: 0.065 ... Validation loss: 0.180[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.4% ... Training loss: 0.062 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.5% ... Training loss: 0.066 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.6% ... Training loss: 0.110 ... Validation loss: 0.257[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.7% ... Training loss: 0.065 ... Validation loss: 0.137[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 90.8% ... Training loss: 0.064 ... Validation loss: 0.134[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.0% ... Training loss: 0.077 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.1% ... Training loss: 0.075 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.2% ... Training loss: 0.064 ... Validation loss: 0.144[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.3% ... Training loss: 0.082 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.4% ... Training loss: 0.078 ... Validation loss: 0.147[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.5% ... Training loss: 0.061 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.6% ... Training loss: 0.072 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.7% ... Training loss: 0.061 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 91.8% ... Training loss: 0.060 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.0% ... Training loss: 0.064 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.1% ... Training loss: 0.062 ... Validation loss: 0.169[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.2% ... Training loss: 0.067 ... Validation loss: 0.157[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.3% ... Training loss: 0.065 ... Validation loss: 0.170[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.4% ... Training loss: 0.062 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.5% ... Training loss: 0.066 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.6% ... Training loss: 0.094 ... Validation loss: 0.244[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.7% ... Training loss: 0.061 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 92.8% ... Training loss: 0.061 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.0% ... Training loss: 0.076 ... Validation loss: 0.174[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.1% ... Training loss: 0.060 ... Validation loss: 0.159[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.2% ... Training loss: 0.076 ... Validation loss: 0.222[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.3% ... Training loss: 0.062 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.4% ... Training loss: 0.067 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.5% ... Training loss: 0.069 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.6% ... Training loss: 0.063 ... Validation loss: 0.165[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.7% ... Training loss: 0.062 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 93.8% ... Training loss: 0.064 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.0% ... Training loss: 0.064 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.1% ... Training loss: 0.071 ... Validation loss: 0.204[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.2% ... Training loss: 0.063 ... Validation loss: 0.160[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.3% ... Training loss: 0.065 ... Validation loss: 0.198[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.4% ... Training loss: 0.066 ... Validation loss: 0.173[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.5% ... Training loss: 0.064 ... Validation loss: 0.167[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.6% ... Training loss: 0.064 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.7% ... Training loss: 0.077 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 94.8% ... Training loss: 0.070 ... Validation loss: 0.211[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.0% ... Training loss: 0.072 ... Validation loss: 0.188[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.1% ... Training loss: 0.062 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.2% ... Training loss: 0.067 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.3% ... Training loss: 0.064 ... Validation loss: 0.150[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.4% ... Training loss: 0.065 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.5% ... Training loss: 0.065 ... Validation loss: 0.168[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.6% ... Training loss: 0.069 ... Validation loss: 0.184[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.7% ... Training loss: 0.063 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 95.8% ... Training loss: 0.063 ... Validation loss: 0.168[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.0% ... Training loss: 0.068 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.1% ... Training loss: 0.077 ... Validation loss: 0.175[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.2% ... Training loss: 0.066 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.3% ... Training loss: 0.068 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.4% ... Training loss: 0.061 ... Validation loss: 0.156[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.5% ... Training loss: 0.068 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.6% ... Training loss: 0.064 ... Validation loss: 0.171[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.7% ... Training loss: 0.072 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 96.8% ... Training loss: 0.068 ... Validation loss: 0.148[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.0% ... Training loss: 0.074 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.1% ... Training loss: 0.063 ... Validation loss: 0.143[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.2% ... Training loss: 0.119 ... Validation loss: 0.178[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.3% ... Training loss: 0.080 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.4% ... Training loss: 0.080 ... Validation loss: 0.201[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.5% ... Training loss: 0.062 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.6% ... Training loss: 0.063 ... Validation loss: 0.184[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.7% ... Training loss: 0.064 ... Validation loss: 0.190[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 97.8% ... Training loss: 0.080 ... Validation loss: 0.232[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.0% ... Training loss: 0.070 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.1% ... Training loss: 0.065 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.2% ... Training loss: 0.060 ... Validation loss: 0.149[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.3% ... Training loss: 0.064 ... Validation loss: 0.158[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.4% ... Training loss: 0.086 ... Validation loss: 0.189[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.5% ... Training loss: 0.078 ... Validation loss: 0.211[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.6% ... Training loss: 0.073 ... Validation loss: 0.142[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.7% ... Training loss: 0.067 ... Validation loss: 0.152[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 98.8% ... Training loss: 0.068 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.0% ... Training loss: 0.060 ... Validation loss: 0.163[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.1% ... Training loss: 0.064 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.2% ... Training loss: 0.064 ... Validation loss: 0.166[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.3% ... Training loss: 0.067 ... Validation loss: 0.162[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.4% ... Training loss: 0.064 ... Validation loss: 0.155[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.5% ... Training loss: 0.134 ... Validation loss: 0.164[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.6% ... Training loss: 0.062 ... Validation loss: 0.153[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.7% ... Training loss: 0.071 ... Validation loss: 0.151[[ 0.          0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-1.33460919 -1.43847501 -1.43847501 ..., -0.19208513 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 1.          1.          1.         ...,  0.          0.          0.        ]]\n",
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651  0.22337816  0.32724398 ...,  0.11951233 -0.19208513\n",
      "  -0.19208513]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Progress: 99.8% ... Training loss: 0.065 ... Validation loss: 0.137"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "### Set the hyperparameters here ###\n",
    "epochs = 900\n",
    "learning_rate = 0.0375\n",
    "hidden_nodes = 26\n",
    "output_nodes = 1\n",
    "\n",
    "N_i = train_features.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for e in range(epochs):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(train_features.index, size=128)\n",
    "    for record, target in zip(train_features.ix[batch].values, \n",
    "                              train_targets.ix[batch]['cnt']):\n",
    "        network.train(record, target)\n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(train_features), train_targets['cnt'].values)\n",
    "    val_loss = MSE(network.run(val_features), val_targets['cnt'].values)\n",
    "    sys.stdout.write(\"\\rProgress: \" + str(100 * e/float(epochs))[:4] \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010904834675989579, 0.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAusAAAH4CAYAAADzU6OVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c131748>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 252,
       "width": 373
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.ylim(ymax=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out your predictions\n",
    "\n",
    "Here, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.01564651 -0.19208513 -0.19208513 ..., -1.23074336 -1.23074336\n",
      "  -1.23074336]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+0AAAIgCAYAAADwRojNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgghhEgo/x8IAKJymq5XJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114e21ef0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 272,
       "width": 502
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions = network.run(test_features)*std + mean\n",
    "ax.plot(predictions[0], label='Prediction')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about your results\n",
    " \n",
    "Answer these questions about your results. How well does the model predict the data? Where does it fail? Why does it fail where it does?\n",
    "\n",
    "> **Note:** You can edit the text in this cell by double clicking on it. When you want to render the text, press control + enter\n",
    "\n",
    "#### How well does the model predict the data?\n",
    "The model predicts the data with an error rate of ~25% based on the validation loss results.\n",
    "#### Where does it fail / why does it fail where it does?\n",
    "The model fails near the Christmas holiday. This is presumably due to human behavioral factors (shopping, spending time with family) around holidays.  The model should potentially account for holiday-impacted days with an additional variable.  Our model only contains a binary holiday variable that fails to account for non-holiday days impacted by holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests\n",
    "\n",
    "Run these unit tests to check the correctness of your network implementation. These tests must all be successful to pass the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [-0.2]\n",
      " [ 0.1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "inputs = [0.5, -0.2, 0.1]\n",
    "targets = [0.4]\n",
    "test_w_i_h = np.array([[0.1, 0.4, -0.3], \n",
    "                       [-0.2, 0.5, 0.2]])\n",
    "test_w_h_o = np.array([[0.3, -0.1]])\n",
    "\n",
    "class TestMethods(unittest.TestCase):\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for data loading\n",
    "    ##########\n",
    "    \n",
    "    def test_data_path(self):\n",
    "        # Test that file path to dataset has been unaltered\n",
    "        self.assertTrue(data_path.lower() == 'bike-sharing-dataset/hour.csv')\n",
    "        \n",
    "    def test_data_loaded(self):\n",
    "        # Test that data frame loaded\n",
    "        self.assertTrue(isinstance(rides, pd.DataFrame))\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for network functionality\n",
    "    ##########\n",
    "\n",
    "    def test_activation(self):\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        # Test that the activation function is a sigmoid\n",
    "        self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5))))\n",
    "\n",
    "    def test_train(self):\n",
    "        # Test that weights are updated correctly on training\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "        \n",
    "        network.train(inputs, targets)\n",
    "        self.assertTrue(np.allclose(network.weights_hidden_to_output, \n",
    "                                    np.array([[ 0.37275328, -0.03172939]])))\n",
    "        self.assertTrue(np.allclose(network.weights_input_to_hidden,\n",
    "                                    np.array([[ 0.10562014,  0.39775194, -0.29887597],\n",
    "                                              [-0.20185996,  0.50074398,  0.19962801]])))\n",
    "\n",
    "    def test_run(self):\n",
    "        # Test correctness of run method\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "\n",
    "        self.assertTrue(np.allclose(network.run(inputs), 0.09998924))\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestMethods())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
